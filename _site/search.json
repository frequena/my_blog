[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco Requena",
    "section": "",
    "text": "Human genetics as a tool for drug discovery\n\n\n\n\n\n\n\ndrug-discovery\n\n\nhuman-genetics\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2022\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nExtracting gene panels from the Genomics England Panelapp\n\n\n\n\n\n\n\nweb-scrapping\n\n\nR\n\n\ngenetics\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2021\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nHow many genes have been associated with cancer in PubMed?\n\n\n\n\n\n\n\nAPI\n\n\nR\n\n\nbias\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2021\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nAn introduction to ROC curves with animated examples\n\n\n\n\n\n\n\nanimation\n\n\nR\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nAn introduction to uncertainty with Bayesian models\n\n\n\n\n\n\n\nanimation\n\n\nR\n\n\nstatistics\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2020\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nPoisson distribution in genomics\n\n\n\n\n\n\n\nanimation\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2020\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nEstimating pi value with Monte Carlo simulation\n\n\n\n\n\n\n\nanimation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2020\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nExploring world flights with networks\n\n\n\n\n\n\n\nnetworks\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2019\n\n\nFrancisco Requena\n\n\n\n\n\n\n  \n\n\n\n\nPrediction of dengue cases through climate variables\n\n\n\n\n\n\n\nmachine-learning\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2017\n\n\nFrancisco Requena\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Postdoctoral Associate at Weill Cornell (New York). My current work focuses on the development of machine learning tools for drug discovery in Alzheimer’s disease. I did my PhD at the Clinical Bioinformatics lab in the Imagine Institute (Paris) where I worked on the development of computational methods, including machine-learning, for the clinical interpretation of variants in rare disease patients."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code ole jeje",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/dengue/dengue.html",
    "href": "posts/dengue/dengue.html",
    "title": "Prediction of dengue cases through climate variables",
    "section": "",
    "text": "Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.\nDrivenData offers different competitions related with multiple types of field, such as health (oh yes!), ecology, society… with a common element: to face the world’s biggest social challenges.\nI decided to join my first competition called ‘DengAI: Predicting Disease Spread’. In this case, the user receives a set of weather information (temperatures, precipitations, vegetations) from two cities: San Juan (Puerto Rico) and Iquitos (Peru) with total cases of dengue by year and week of every year.\n\n\n\n\n\nThe goal of the competition is to develop a prediction model that would be able to anticipate the cases of dengue in every city depending on a set of climate variables.\nThe DrivenData’s blog wrote some days ago, a post about a fast approach with this dataset. It was written in Python. So, I decided to “translate” to R language.\nThe next code is divided into three main points:\n1. Code with clean tasks (transform NA values, remove of columns…) and exploratory analyses.\n2. Function with every step during cleaning of data.\n3. Development of model, prediction and comparison of predicted vs real total cases detected.\n\n# Load of libraries\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(corrplot)\nlibrary(MASS)\nlibrary(reshape2)\n\n\n# Load data\ntrain_features <- read.csv('data/dengue_features_train.csv')\n  \ntrain_labels <- read.csv('data/dengue_labels_train.csv')\ntest_features <- read.csv('data/dengue_features_test.csv')\nsubmission_format <- read.csv('data/submission_format.csv')\n  \n# Filter of data by city  \nsj_train_labels <- filter(train_labels, city == 'sj')\nsj_train_features <- filter(train_features, city == 'sj')\niq_train_labels <- filter(train_labels, city == 'iq')\niq_train_features <- filter(train_features, city == 'iq')\n\n\n# Is there NA values?\ndf_na_sj <- as.data.frame(apply(sj_train_features,2, function(x) any(is.na(x))))\ncolnames(df_na_sj) <- 'is_there_NA'\ndf_na_sj$number_NA <- apply(sj_train_features,2, function(x) sum(is.na(x)))\ndf_na_sj$mean_NA <- apply(sj_train_features, 2, function(x) mean(is.na(x)))\ndf_na_iq <- as.data.frame(apply(iq_train_features, 2, function(x) any(is.na(x))))\ncolnames(df_na_iq) <- 'is_there_NA'\ndf_na_iq$number_NA <- apply(iq_train_features, 2, function(x) sum(is.na(x)))\ndf_na_iq$mean_NA <- apply(iq_train_features, 2, function(x) mean(is.na(x)))\n\n\n# Vegetation Index over Time Plot with NAs\nggplot(sj_train_features, aes(x = as.Date(week_start_date), y = ndvi_ne )) +\n  ggtitle('Vegetation Index over Time') +\n  theme_bw() +\n  xlab('Title') +\n  geom_line(na.rm = FALSE, color = 'blue') +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n# Remove 'weekofyear' column\nsj_train_features <- dplyr::select(sj_train_features, -week_start_date)\niq_train_features <- dplyr::select(iq_train_features, -week_start_date)\n# Fill the NA values with the previous value\nsj_train_features <- sj_train_features %>%\n            do(na.locf(.))\niq_train_features <- iq_train_features %>%\n            do(na.locf(.))\n# Distribution of labels\n# print(mean(sj_train_labels$total_cases))\n# print(var(sj_train_labels$total_cases))\n# \n# print(mean(iq_train_labels$total_cases))\n# print(var(iq_train_labels$total_cases))\nggplot(sj_train_labels, aes(x = total_cases)) +\n  theme_bw() +\n  ggtitle('Cases of dengue in San Juan') +\n  geom_histogram() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nggplot(iq_train_labels, aes(x = total_cases)) +\n  theme_bw() +\n  ggtitle('Cases of dengue in Iquitos') +\n  geom_histogram() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n# Add total_cases column to *_train_features dataframes\n# sj_train_features <- left_join(sj_train_features, sj_train_labels, by = c('city', 'year', 'weekofyear'))\n sj_train_features$total_cases <- sj_train_labels$total_cases\n# iq_train_features <- left_join(iq_train_features, iq_train_labels, by = c('city', 'year', 'weekofyear'))\niq_train_features$total_cases <- iq_train_labels$total_cases\n# Correlation matrix\nm_sj_train_features <- data.matrix(sj_train_features)\nm_sj_train_features <- cor(x = m_sj_train_features[,3:24], use = 'complete.obs', method = 'pearson')\nm_iq_train_features <- data.matrix(iq_train_features)\nm_iq_train_features <- cor(x = m_iq_train_features[,3:24], use = 'everything', method = 'pearson')\n# Correlation Heatmap\ncorrplot(m_sj_train_features, type = 'full', tl.col = 'black', method=\"shade\")\n\n\n\ncorrplot(m_iq_train_features, type = 'full', tl.col = 'black', method = 'shade')\n\n\n\n# Correlation Bar plot\ndf_sj_train_features <- data.frame(m_sj_train_features)[2:21,] \ndf_sj_train_features <- dplyr::select(df_sj_train_features, total_cases) \n                                    \ndf_iq_train_features <- data.frame(m_iq_train_features)[2:21,]\ndf_iq_train_features <- dplyr::select(df_iq_train_features, total_cases) \nggplot(df_sj_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +\n  geom_bar(stat = 'identity') +\n  theme_bw() +\n  ggtitle('Correlation of variables in San Juan') +\n  ylab('Correlation') +\n  xlab('Variables') +\n  coord_flip()\n\n\n\nggplot(df_iq_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +\n  geom_bar(stat = 'identity') +\n  theme_bw() +\n  ggtitle('Correlation of variables in Iquitos') +\n  ylab('Correlation') +\n  xlab('Variables') +\n  coord_flip()\n\n\n\n\n\n# Function data cleaning\ndata_clean <- function(df_dengue_features, df_dengue_labels = NULL, add_cases = TRUE) {\n  \n  # Filter by city\n  sj_df_dengue_features <- filter(df_dengue_features, city == 'sj')\n  iq_df_dengue_features <- filter(df_dengue_features, city == 'iq')\n  \n  if (add_cases == TRUE) {\n  sj_df_dengue_labels <- filter(df_dengue_labels, city == 'sj')\n  iq_df_dengue_labels <- filter(df_dengue_labels, city == 'iq')\n  }\n  # Removing week_start_date column\n  sj_df_dengue_features <- dplyr::select(sj_df_dengue_features, -week_start_date)\n  iq_df_dengue_features <- dplyr::select(iq_df_dengue_features, -week_start_date)\n  # Fill of NA values with the previous value\n  sj_df_dengue_features <- sj_df_dengue_features %>%\n    do(na.locf(.))\n  \n  iq_df_dengue_features <- iq_df_dengue_features %>%\n    do(na.locf(.))\n  \n  # Add total_cases to dataframe with features\n  if (add_cases == TRUE) {\n  sj_df_dengue_features$total_cases <- sj_df_dengue_labels$total_cases\n  iq_df_dengue_features$total_cases <- iq_df_dengue_labels$total_cases\n  }\n  \n  # Converting character columns into numbers\n  sj_df_dengue_features <- as.data.frame(apply(sj_df_dengue_features,2,as.numeric))\n  sj_df_dengue_features$city <- rep('sj', nrow(sj_df_dengue_features))\n  iq_df_dengue_features <- as.data.frame(apply(iq_df_dengue_features,2,as.numeric))\n  iq_df_dengue_features$city <- rep('iq', nrow(iq_df_dengue_features))\n  \n  result <- list(sj_df_dengue_features, iq_df_dengue_features )\n  \n  return(result)\n}\n\n\n# Getting data_training clean\ndata_train <- data_clean(train_features, train_labels, TRUE)\n# Getting negative binomials models by city\ntraining_sj <- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +\n                     reanalysis_dew_point_temp_k +\n                     station_min_temp_c +\n                     station_avg_temp_c, data = data_train[[1]])\ntraining_iq <- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +\n                        reanalysis_dew_point_temp_k +\n                        station_min_temp_c +\n                        station_avg_temp_c, data = data_train[[2]])\n# Getting data_test clean\ndata_test <- data_clean(test_features, add_cases = FALSE)\n# Testing model with training data\nprediction_train_sj <-  predict(training_sj, data_train[[1]], type = 'response')\nprediction_train_iq <-  predict(training_iq, data_train[[2]], type = 'response')\ndf_prediction_train_sj <- data.frame('prediction' = prediction_train_sj, 'actual' = data_train[[1]]$total_cases,\n                                     'time' = as.Date(train_features$week_start_date[1:936]))\ndf_prediction_train_sj <- melt(df_prediction_train_sj, id.vars = 'time')\nggplot(df_prediction_train_sj, aes(x = time, y = value, color = variable)) +\n  geom_line() +\n  ggtitle('Dengue predicted Cases vs. Actual Cases (City-San Juan) ')\n\n\n\ndf_prediction_train_iq <- data.frame('prediction' = prediction_train_iq, 'actual' = data_train[[2]]$total_cases,\n                                     'time' = as.Date(train_features$week_start_date[937:1456]))\ndf_prediction_train_iq <- melt(df_prediction_train_iq, id.vars = 'time')\nggplot(df_prediction_train_iq, aes(x = time, y = value, color = variable)) +\n  geom_line() +\n  ggtitle('Dengue predicted Cases vs. Actual Cases (City-Iquitos) ')\n\n\n\n# Prediction of total_cases in the data set\nprediction_sj <-  predict(training_sj, data_test[[1]], type = 'response')\nprediction_iq <-  predict(training_iq, data_test[[2]], type = 'response')\n \ndata_prediction_sj <- data.frame('city' = rep('sj', length(prediction_sj) ), \n                                 'total_cases' = prediction_sj, \n                                 'weekofyear' = data_test[[1]]$weekofyear,\n                                 'year' = data_test[[1]]$year )\ndata_prediction_iq <- data.frame('city' = rep('iq', length(prediction_iq) ), \n                                 'total_cases' = prediction_iq,\n                                 'weekofyear' = data_test[[2]]$weekofyear,\n                                 'year' = data_test[[2]]$year)\n  \nsubmission_format$total_cases <- as.numeric(c(data_prediction_sj$total_cases, \n                                                   data_prediction_iq$total_cases))\nsubmission_format$total_cases <- round(submission_format$total_cases, 0)\n  \nwrite.csv(submission_format,\n          file = 'submission_format_submit.csv', row.names = F)"
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html",
    "href": "posts/flights_networks/exploring_world_flights_networks.html",
    "title": "Exploring world flights with networks",
    "section": "",
    "text": "Recently, I started to read this free accessible book written by Albert-László Barabási. In the Chapter 4 of his book, it depicts the USA airport networks to represent scale-free networks. I was wondering if we can get a world picture, creating the same network but including the global routes using open data from internet."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#what-is-a-scale-free-network",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#what-is-a-scale-free-network",
    "title": "Exploring world flights with networks",
    "section": "1. What is a scale-free network?",
    "text": "1. What is a scale-free network?\nScale-free networks are characterized by a large number of nodes with low degree (number of links) and very few hubs with a high degree. If we represent the distribution of degrees of these nodes, it follows a power-law distribution. To illustrate this idea, let’s create a quick example:\n\n# Load libraries\nlibrary(tidygraph) \nlibrary(ggraph)\nlibrary(igraph)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggthemes)\n# 1. Example showing a scale-free network\nscale_free_net <- play_barabasi_albert(n = 1000, power = 1)\n# 1.1 Scale-free network\np1 <- ggraph(scale_free_net, layout = 'kk') + \n  geom_edge_link(alpha = 0.3) + \n  geom_node_point(fill = 'steelblue', color = 'black', shape = 21) +\n  ggtitle('Scale-free network') + \n  theme_graph()\nvector_values <- degree_distribution(scale_free_net)[-1] # Eliminate first element, it represents zero degree vertices\n  \ndf <- data.frame(frequency = vector_values,\n                 degrees = seq(1, length(vector_values),1))\n# 1.2 Degree distribution\np2 <- ggplot(df, aes(degrees, frequency)) +\n  geom_col(fill = 'steelblue', color = 'black') +\n  ggtitle('Degree Distribution of a scale-free network') +\n  ylab('Relative frequency') +\n  xlab('Number of links') +\n  theme_bw()\np1 + p2\n\n\n\n\nMany real networks share this feature. For instance, if we take a look how internet is organized and calculate the number of links that every site has, we find that the most of websites (nodes) have a low number of links (edges) and very few will have a large number of links (e.g. Google, Facebook…). Other examples are social, co-authorship or protein-protein network.\nWe hope to see the same pattern through our airport’s network: very few airports have a large number of routes while the most will have few routes."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#data-airports-and-routes",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#data-airports-and-routes",
    "title": "Exploring world flights with networks",
    "section": "2. Data (airports and routes)",
    "text": "2. Data (airports and routes)\nAt the beginning, we talked about creating our own network of airlines routes. To achieve this, we download our data from Openflights whose have a lot of information about flights. We will just download data about airports (selecting: code, longitude and latitude) and routes (selecting: name, code source, code destination and continent location). Besides, we will clean those observations with NA’s values or wrong strings.\nImportant: Aiports will be the nodes of our network and the routes will conform the edges between the nodes.\n\n# Data with routes\n# https://openflights.org/data.html#route\ndf <- read.csv('data/routes.dat',header = FALSE,\n               stringsAsFactors = FALSE,\n               col.names = c('airline', 'airline_id', 'src', 'src_id', 'dest', 'dest_id', 'codeshare','stops',  'equip'))[,c(3,5)]\n# Data with airport information\n# https://openflights.org/data.html#airport\ndf2 <- read.csv('data/airports.dat',header = FALSE,\n                   stringsAsFactors = FALSE)[,c(2,5,7,8,12)]\ncolnames(df2) <- c('name','code', 'lat', 'long', 'location')\n# Clean data\ndf_airport <- df2 %>% \n  filter(!str_detect(code, fixed(\"\\\\N\"))) %>%\n  filter(!str_detect(location, fixed(\"\\\\N\"))) %>%\n  as_tibble()\ntmp_loc <- str_split(df_airport$location, '/')\ndf_airport$location <- map_chr(tmp_loc, function(x) x[[1]])\ndf_airport <- df_airport %>% mutate(location = as.factor(location))\ndf_routes <- df %>% \n  filter(!str_detect(src, fixed(\"\\\\N\")) & !str_detect(dest, fixed(\"\\\\N\"))) %>%\n  filter(!src == dest) %>%\n  group_by(src, dest) %>%\n  count() %>%\n  arrange(desc(n)) %>%\n  ungroup() %>%\n  as_tibble()"
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#airport-network-visualization",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#airport-network-visualization",
    "title": "Exploring world flights with networks",
    "section": "3. Airport Network visualization",
    "text": "3. Airport Network visualization\nTo make possible the downstream analysis, we have to transform the observations of our dataframe into nodes and edges (tbl_graph object). We can do this thanks to the package ggraph. Once we do this, we will be able to visualise the network applying different algorithms layers and calculate topological parameters of the nodes that otherwise would not be possible.\nFor instance, we can choose the ‘mds’ layout (you can find many other layouts described here). This algorithm layout measures the shortest path between each node and display together those nodes which are closer in the network. Besides, we are going to calculate some scores per node and to make faster the algorithm, I will eliminate those airports whose number of routes are low.\n\n# Convert dataframe (df_routes) to tbl_graph object (df_graph)\ndf_graph <- as_tbl_graph(df_routes,directed = FALSE) %>% activate(edges) %>%\n  filter(!edge_is_multiple()) %>% activate(nodes) %>%\n  mutate(n_degree = centrality_degree(),\n         betweenness = centrality_betweenness(),\n         community = group_walktrap(),\n         n_triangles = local_triangles(),\n         clust = local_transitivity()) %>%\n  left_join(df_airport, by = c('name' = 'code')) %>%\n  filter(!is.na(lat) & !is.na(long))\n\nFALSE Warning in betweenness(graph = graph, v = V(graph), directed = directed, :\nFALSE 'nobigint' is deprecated since igraph 1.3 and will be removed in igraph 1.4\n\n# ggraph(df_graph %>% activate(nodes) %>% filter(n_degree >= 10), layout = \"mds\") + \n#   geom_edge_link(aes(edge_width = n), alpha = 0.1, edge_colour = 'gray') + \n#   geom_node_point(aes(size = n_degree, fill = location), shape = 21) +\n#   scale_fill_brewer(palette = 'Set1') +\n#   scale_size(range = c(0, 14)) +\n#   theme_graph() +\n#   guides(size=FALSE, edge_width = FALSE, fill = guide_legend(override.aes = list(size = 7))) +\n#   ggtitle('Airports network')\n\nBesides, we plot the degree distribution of our network using ggplot2. For that, we convert our tbl_graph to a dataframe (the reverse step we did before) applying the function activation(nodes) and then as_tibble().\n\n# Degree distribution\ndf_nodes <- df_graph %>% activate(nodes) %>% as_tibble()\nggplot(df_nodes, aes(n_degree)) +\n  geom_histogram(fill = 'steelblue', color = 'black', binwidth = 1) +\n  ggtitle('Degree Distribution of airports network') +\n  ylab('Frequency') +\n  xlab('Number of links') +\n  theme_bw()\n\n\n\n\nAs we saw at the beginning, both networks follow a power-law distribution."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#where-is-my-airport",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#where-is-my-airport",
    "title": "Exploring world flights with networks",
    "section": "4. Where is my airport?",
    "text": "4. Where is my airport?\nAt first glance, let’s take a look at the distribution of the airports around the world based on their region:\n\nworldmap <- borders(\"world\", colour=\"#efede1\", fill=\"#efede1\") \n# Get airports by degree\nggplot(df_airport, aes(long, lat)) + worldmap + \n  geom_point(aes(fill = location), color = 'black', shape = 21) +\n  theme_void() +\n  guides(fill = guide_legend(override.aes = list(size = 7))) +\n  ggtitle(' Aiports across the world by region')\n\n\n\n\nWe can see the biggest hubs are influenced by the economical situation and the population density of the region."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-longest-path-possible",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-longest-path-possible",
    "title": "Exploring world flights with networks",
    "section": "What is the longest path possible?",
    "text": "What is the longest path possible?\nCan you guess how many steps would be required to travel the longest path possible between two airports? This number is called diameter and can be calculated easily:\n\ndf_graph %>% activate(nodes) %>% \n  mutate(diam = graph_diameter()) %>% \n  distinct(diam) %>% \n  as_tibble()\n\n# A tibble: 1 × 1\n   diam\n  <dbl>\n1    12\n\n\nThe longest path is 12 steps. Not so long if we take into account the remote distance of some of the airports (Siberia, Greenland, Pacific regions…)."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-shortest-path-between-two-airports",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-shortest-path-between-two-airports",
    "title": "Exploring world flights with networks",
    "section": "What is the shortest path between two airports?",
    "text": "What is the shortest path between two airports?\nWe can select an airport and calculate the shortest path needed to reach another one. For instance, the Charles de Gaulle Airport (Paris) is one step from Adolfo Suárez Madrid–Barajas (Madrid), but what is the number of steps needed to reach the Hawai’s airport from Paris? Let’s calculate it:\n\nshortest_paths(df_graph, 'CDG', 'HNL')$vpath[[1]]\n\n+ 3/3209 vertices, named, from 74156b1:\n[1] CDG ORD HNL\n\n\nThe shortest path from Paris to Honolulu is: Paris -> Chicago -> Honolulu.\nNow, imagine that we calculate all the shortest paths between Paris and the rest of airports and we repeat it with every airport and calculate the average. This value is called: average shortest path and is average number of minimum connections required from any airport to any other airport.\n\ndf_graph %>% activate(nodes) %>% \n  mutate(dist = graph_mean_dist()) %>% \n  distinct(dist) %>% \n  as_tibble()\n\n# A tibble: 1 × 1\n   dist\n  <dbl>\n1  3.97\n\n\nThe average shortesth path is 3.94, almost 4 steps on average to go from an airport to any other."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-longest-distance-possible-from-a-specific-airport",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#what-is-the-longest-distance-possible-from-a-specific-airport",
    "title": "Exploring world flights with networks",
    "section": "What is the longest distance possible from a specific airport?",
    "text": "What is the longest distance possible from a specific airport?\nWe are in Paris again, and we want to go to the most distant airport possible (in steps). This value is called eccentricity and is specific for each airport. Let’s take a look at three of the most connected airports:\n\ndf_graph_eccen <- df_graph %>% activate(nodes) %>% \n  mutate(eccentricity = node_eccentricity()) %>% as_tibble()\n  \ndf_graph_eccen %>% \n  filter(name == 'ATL' | name == 'CDG' | name == 'AMS') %>% \n  select(name.y, eccentricity )\n\n# A tibble: 3 × 2\n  name.y                                           eccentricity\n  <chr>                                                   <dbl>\n1 Hartsfield Jackson Atlanta International Airport            7\n2 Charles de Gaulle International Airport                     7\n3 Amsterdam Airport Schiphol                                  7\n\n\nWe would need 7 steps to go from Paris to the most distant airport, the same value obtained with Atlanta and Amsterdam airports. This make sense as we have selected nodes with the highest nº of routes. But the value 7 is the lowest that we can get?\nLet’s see the distribution:\n\n# The filter(eccentricity > 2) eliminate those airports that are disconnected from the main network and have a eccentricity from 0 to 2\nggplot(df_graph_eccen %>% filter(eccentricity > 2), aes(eccentricity)) +\n  geom_histogram(fill = 'steelblue', color = 'black') +\n  ylab('Nº of airports') +\n  theme(text = element_text(size=20))\n\n\n\n\nAs we see above, most of the airports are located between 8 and 9. Those airports with the highest number of routes have a value of 7. But there is an airport whose value is 6.\n\n df_graph_eccen %>% filter(eccentricity == 6)\n\n# A tibble: 1 × 11\n  name  n_deg…¹ betwe…² commu…³ n_tri…⁴ clust name.y   lat  long locat…⁵ eccen…⁶\n  <chr>   <dbl>   <dbl>   <int>   <dbl> <dbl> <chr>  <dbl> <dbl> <fct>     <dbl>\n1 YYZ       147 249941.       1    2061 0.192 Leste…  43.7 -79.6 America       6\n# … with abbreviated variable names ¹​n_degree, ²​betweenness, ³​community,\n#   ⁴​n_triangles, ⁵​location, ⁶​eccentricity\n\n\nWell, it is interesting that the airport with the lowest eccentricity is Lester B. Pearson International Airport located at Toronto. Its number of routes (n_degree) is not very high but has an important particularity. If we see the map, Canada is a country with a large number of airports sparse along the territory. While the majority of airports have to “spend” steps to reach those distant airport (mainly at the north of the territory), this airport is very close to them and at the same time is close to the rest of airports across the world (USA, Europe, China…)"
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#where-are-the-hubs",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#where-are-the-hubs",
    "title": "Exploring world flights with networks",
    "section": "Where are the hubs?",
    "text": "Where are the hubs?\nWe can detect also the most relevant hubs (densely connected subgraphs) and display those airports that belongs to one of the top 10 hubs:\n\nggplot(df_nodes %>% filter(community <= 10), aes(long, lat)) + worldmap + \n  geom_point(aes(fill = as.factor(community)), color = 'black', shape = 21) +\n  theme_void() +\n  scale_fill_brewer(palette = 'Paired') +\n  guides(fill = guide_legend(override.aes = list(size = 12))) +\n  ggtitle(' Aiports across the world by region') +\n  labs(fill=\"List of Hubs\") +\n  theme_map()\n\n\n\n\nWe have applied a walktrap community finding algorithm that uses random walks between the nodes and group those airports that are connected by short random walks.\nIf you take a look at the map, these hubs represent not only a group of airports densely connected but also political and economical hubs. For instance, a hub includes Ex-soviets states, another Europe, Canary Islands and some cities from Magreb.\nIn addition, we can classify the airports in 3 categories:\n\nCore: Those aiports whose have the highest number of triangles (subgraph of 3 nodes and 3 edges). If an airport is located in many triangles, we consider it as a well connected airport.\nPeryphery: Airports that are located in distant regions with few routes.\nBridge: Those airports that allow the communication between the airports that form the core and the periphery.\n\n\ndf_nodes <- df_nodes %>% mutate(category = 'Bridge')\ndf_nodes$category <- ifelse(df_nodes$n_triangles > 400, 'Core', df_nodes$category)\ndf_nodes$category <- ifelse(df_nodes$clust == 0, 'Periphery', df_nodes$category)\nggplot(df_nodes, aes(long, lat)) + worldmap +\n  geom_point(aes(fill = category), color = 'black', shape = 21) +\n  facet_grid(category ~.) +\n  theme_map() +\n  theme(strip.text = element_text(size=25)) +\n  guides(fill = guide_legend(override.aes = list(size = 20)))"
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#routes-by-number-of-airlines",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#routes-by-number-of-airlines",
    "title": "Exploring world flights with networks",
    "section": "7. Routes by number of airlines",
    "text": "7. Routes by number of airlines\nWe can take a look at those routes whose have the largest number of airlines:\n\nggplot(df_routes %>% top_n(20, n), aes(reorder(paste(src, dest, sep =' - '), -n), n)) +\n  geom_col(aes(fill = n),  color = 'black') +\n  scale_fill_viridis() +\n  ggtitle('Top 20 routes by number of airlines') +\n  ylab('Frequency') +\n  xlab('Route') +\n  theme_bw() +\n  guides(fill = FALSE) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        text = element_text(size=20))\n\nFALSE Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\nFALSE \"none\")` instead."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#conclusion",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#conclusion",
    "title": "Exploring world flights with networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe relevance of an airport in the network can be assessed through different metrics: nº of routes, nº of triangles, clustering, betweenness, eccentricity or shortest path. At the same time, the identification of groups of airports, we have clustered airports by continent, random walks algorithm, or using a blend of centrality measures filtering the nodes in three groups (core, bridge, peripherial).\nIn conclusion, network science allows us to improve our knowledge about data that can be converted into a network, through the use of multiple approaches."
  },
  {
    "objectID": "posts/flights_networks/exploring_world_flights_networks.html#final-notes",
    "href": "posts/flights_networks/exploring_world_flights_networks.html#final-notes",
    "title": "Exploring world flights with networks",
    "section": "Final notes",
    "text": "Final notes\n\nTo simplify this post, I have not included the direction of the edges neither the real distance between airports.\nA very interesting point is the analysis of the resilence: what would happen if we delete a specific airport from the network? Would the impact be equal across the aiports?"
  },
  {
    "objectID": "posts/montecarlo_pi_value/montecarlo_pi_value.html",
    "href": "posts/montecarlo_pi_value/montecarlo_pi_value.html",
    "title": "Estimating pi value with Monte Carlo simulation",
    "section": "",
    "text": "# Load of libraries\nlibrary(tidyverse)\nlibrary(sp)\nlibrary(gganimate)\n\n\nn_simulations <- 3000\ndf <- tibble(\n  \n  values_x = runif(n_simulations,0,1),\n  values_y = runif(n_simulations,0,1)\n)\ncircleFun <- function(center=c(0,0), diameter=1, npoints=100, start=0, end=2)\n{\n  tt <- seq(start*pi, end*pi, length.out=npoints)\n  data.frame(x = center[1] + diameter / 2 * cos(tt), \n             y = center[2] + diameter / 2 * sin(tt))\n}\ndat <- circleFun(c(0,0), 2, start=1.5, end=2.5)\ndf <- df %>%\n  rowwise() %>%\n  mutate(label = point.in.polygon(values_x, values_y, dat$x, dat$y, mode.checked=FALSE)) %>%\n  ungroup() %>%\n  mutate(count_in = cumsum(label),\n         id = row_number(),\n         pi_value = 4*(count_in / id))\n\n\nggplot(df) +\n  geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), \n            colour = \"black\", show.legend = FALSE) +\n  geom_polygon(aes(x, y), data = dat, alpha = 0.4) +\n  geom_point(aes(x = values_x, y = values_y, fill = factor(label), group=id), \n             color = 'black', shape = 21, show.legend = FALSE) +\n  theme_minimal() +\n  coord_cartesian(ylim=c(0, 1), xlim = c(0,1)) +\n  transition_reveal(id)  +\n  labs(title = 'Nº of observations: {frame_along} of {n_simulations}',\n       subtitle = 'Estimated value of pi: {df$pi_value[as.integer(frame_along)]}')\n\n\n\n\n\nggplot(df, aes(id, pi_value)) +\n  geom_line() +\n  geom_point(colour = 'red', size = 3) +\n  transition_reveal(id) +\n  geom_hline(yintercept = pi, color = 'red', linetype= 'dashed') +\n  theme_bw() +\n  labs(title = 'Nº of observations: {frame_along} of {n_simulations}',\n       subtitle = 'Estimated value of pi: {df$pi_value[as.integer(frame_along)]}',\n       x = 'Sample size',\n       y = 'Pi value') +\n    coord_cartesian(ylim=c(0, 4))"
  },
  {
    "objectID": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html",
    "href": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html",
    "title": "Poisson distribution in genomics",
    "section": "",
    "text": "In this post, I will discuss briefly what is the Poisson distribution and describe two examples extracted from research articles in the genomics field. One of them based on the distribution of structural variants across the genome and other about de novo variants in a patient cohort."
  },
  {
    "objectID": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#poisson-distribution",
    "href": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#poisson-distribution",
    "title": "Poisson distribution in genomics",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nIn genomics, many of the events we observe correspond to countable values. For instance, the number of mutations found in a specific type of genomic regions or a patient cohort, sequence reads…The Poisson distribution is a discrete probability model that takes countable numbers as the mentioned before and will be defined as events.\nTo calculate a Poisson distribution, we need to specify a single parameter which is called lambda (\\[\\lambda\\]). This value is known as the rate parameter and defines the mean number of events in a given interval. In other words, if we know the total number of events of our system, we just need to divide it by the number of intervals. We will see further some examples of this.\nOnce we know lambda, we can calculate the probability of seeing \\[/x\\] number of events on a given interval, following this formula:\n\\[P\\left( x \\right) = \\frac{{e^{ - \\lambda } \\lambda ^x }}{{x!}}\\]\nIn the next example, we are going to generate a Poisson distribution of 100 samples whose lambda value is equal to 2 with the rpois function:\n\n# Load libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tidyr)\ntibble(x = rpois(n = 100, lambda = 2)) %>%\n ggplot(aes(x)) +\n geom_histogram(binwidth = 1, fill = 'steelblue', color = 'black') + \n theme_bw() +\n labs(title = 'Poisson distribution', x = 'Events', y = 'Count')\n\n\n\n\nAn interesting aspect about the Poisson distribution: the mean and variance of the distribution are equal to the value of lambda. Therefore, the probability of finding an interval with 3 events (for instance) is higher as long as we increase the value of lambda.\nConfusing? Check out this example:\n\npoisson_tbl <- tibble(lambda = seq(1.2, 7, 0.2)) %>% \n rowwise() %>%\n mutate(value = paste(rpois(1000, lambda), collapse = ',')) %>%\n separate_rows(value, sep = ',') %>%\n mutate(value = as.integer(value)) %>%\n mutate(prob_7 = round(dpois(7, lambda), 5))\npoisson_tbl %>%\n ggplot(aes(value)) +\n geom_histogram(binwidth = 1, fill = 'steelblue', color = 'black') +\n transition_states(lambda) +\n geom_vline(xintercept = 7, color = 'red', linetype = 4 ) +\n labs(title = 'Poisson distribution', \n  subtitle = 'lambda: {closest_state}, P(X = 7) : {poisson_tbl[poisson_tbl$lambda == {closest_state},] %>% pull(prob_7) %>% unique()}',\n  x = 'Events',\n  y = 'Count') +\n theme_bw()\n\nWarning: No renderer available. Please install the gifski, av, or magick package\nto create animated output\n\n\nNULL\n\n\nIn the example above, we are generating 30 times a set of 1000 random values following a Poisson distribution, increasing each time the value of lambda (from 1.2 to 7). The probability of finding an interval with 7 events (red line) is higher as long as we increase lambda.\nAs we said before, we only need lambda to generate a Poisson distribution. Generally, we calculate this value if we know beforehand the number of events and intervals (as we will see in the second example, this is not always the case).\nLet’s put some examples of what an interval or event can be:\n\nIntervals. We can define intervals as fixed time units, such as days, months, years…or also delimited areas of a geographical region (see this nice post of cancer clusters or this one about the distribution of impacts of V-1 and V-2 missiles during WWII).\nEvents. The amount of clicks on a banner or the number of homicides or blackouts every year…An important condition is that each event is independent of each other (events occur independently).\n\nEvery time, we perform a Poisson distribution, we always ask ourselves the same question: are these events distributed randomly across the intervals?"
  },
  {
    "objectID": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#poisson-distribution-in-genomics",
    "href": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#poisson-distribution-in-genomics",
    "title": "Poisson distribution in genomics",
    "section": "Poisson distribution in genomics",
    "text": "Poisson distribution in genomics\nIn genomics, as many other fields, there are different ways to define intervals and events. In the next examples, we will explore two completely different approaches:\n\nExample 1 : Structural variants in the human genome\nStructural Variants (SVs) are mutations of more than 50bp and include deletions, duplications, inversion, translocations…These types of variants are important causes of multiple disorders, such as autism, schizoprenia, autoimmune diseases or developmental disorders.\nIn this article Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots [1], the authors explore whether the distribution of this type of mutations is random or follow any pattern across the genome.\nTo do this, the researchers defined each structural variant as an event. Next, they divided the human genome into 100 kb intervals and after discarding incomplete intervals (the intervals need to be fixed), they got a total of 28,103 intervals.\nThe number of structural variants is 42,758 SVs. Therefore, to calculate lambda, they just had to divide this number by the total number of intervals. Finally, they generated a Poisson distribution and defined as “hotspot regions” all the intervals that exceeded the 99th percentile (6 SVs per 100 kb interval) concluding that these intervals had more SVs than expected by chance. Furthermore, they were able to identify “desert regions” as those intervals with a lower nº of SVs as compared with the number expected by chance.\n\n\nExample 2 : De novo variants in neurodevelopmental disorders\nIn this article De novo mutations in regulatory elements in neurodevelopmental disorders [2], the researchers explore the impact of de novo variants (those present on children but not their parents) on regulatory regions of the genome in a cohort of patients with neurodevelopmental disorders. The majority of patients in this cohort did not present any de novo mutations (DNMs) in protein-coding genes. Therefore, a plausible hypothesis is to find some of these DNMs in those regions of the DNA yet unexplored: regulatory regions.\nMost of the human genome (98%) do not encode for protein regions. Therefore, the researchers decided to narrow down the search and focus only on those regulatory regions based on two features: regions highly conserved or experimentally validated.\nFinally, they found DNMs mapping this set of regulatory regions, which is great since it allows us to identify the causal mutation and find a diagnos….but wait a minute: Each person’s genome harbors many variants and most of the time, these variants are not harmful. So, we expect to find variants randomly in these regulatory regions just by chance. Yes, as you can guess…here it comes the Poisson distribution.\nThe researchers knew this fact, therefore, to validate their results, they performed (surprise…) a Poisson distribution. First, they calculated the lambda parameter following the next approach:\nThey focused on 6,239 individuals and counted the number of mutations found in regulatory regions. Furthermore, for each region, they calculated the expected number of mutations given the nucleotide context.\nOnce they got the expected number of mutations for each regulatory region, they summed the values and multiplied by the total number of individuals (6,239) to obtain lambda. This value represents the expected number of mutations. Finally, they generated a Poisson distribution with lambda and compared it with the number of observed mutations. This allowed them to demonstrate, first, there were some subgroups of regulatory regions with an excess of the novo variants and second, this excess could be considered as statistically significative. These significant regions were mostly featured by fetal brain DNase signal.\nContrary to what we saw at the beginning of the post, the approach to calculate lambda has been completely different in the second example. Precisely, this versatility makes the Poisson distribution one of the most popular ways to model counted data."
  },
  {
    "objectID": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#notes",
    "href": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#notes",
    "title": "Poisson distribution in genomics",
    "section": "Notes",
    "text": "Notes\n\nWe discussed here about two different scenarios whose events were defined as mutations. But the Poisson distribution can help us to modelate other kind of events, for instance, sequence data. One of the most used techniques for the identification of peaks in Chip-seq analysis is called Model-based Analysis of ChIP-Seq data ( MACS). This program generates a Poisson distribution to identify regions with a higher number of reads than just by chance.\nWhen we use the genome to produce fixed size intervals to generate a Poisson distribution, an important aspect, it is the genome size. In principle, we already know this value: ~3,100 milions b.p (hg19) and ~ 3,200 millions b.p (hg38). Unfortunately, there are many inaccesible regions (gap regions) represented by Ns. Therefore, the use of the total size would artificially decrease the value of lambda and increase the number of false findings. As a consequence, we need to provide a effective genome size."
  },
  {
    "objectID": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#references",
    "href": "posts/poisson-distribution-genomics/poisson-distribution-genomics.html#references",
    "title": "Poisson distribution in genomics",
    "section": "References",
    "text": "References\n[1] Lin, Yen-Lung, and Omer Gokcumen. “Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots” Genome biology and evolution 11.4 (2019): 1136-1151.\n[2] Short, Patrick J., et al. “De novo mutations in regulatory elements in neurodevelopmental disorders.” Nature 555.7698 (2018): 611-616."
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html",
    "href": "posts/roc-curves/roc-curves-animated-examples.html",
    "title": "An introduction to ROC curves with animated examples",
    "section": "",
    "text": "Receiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance. Possibly, because we are used to interpreting information as single values, such as mean, median, accuracy…ROC curves are different because it represents a group of values conforming a curve. Besides, it is the most popular way to represent a model performance for a particular dataset where the task is a binary classification.\nBefore explaining where the ROC curves come from, let’s focus on what is the outcome of most of the classification models. To illustrate this point, let’s train a few logistic regression models with a toy dataset and use the package parsnip which provides a common interface to train models from many other packages."
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#data",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#data",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Data",
    "text": "Data\nFor this post, we are going to use a dataset that includes 310 patients and six explanatory variables related to biomechanical features of the vertebral column. Besides, it contains a response variable abnormality that defines if the patient has been diagnosed with a medical condition in the vertebral column (yes and no).\n\nlibrary(tidyverse)\nlibrary(gganimate) # animated plots\nlibrary(magick) # combine two gif\nlibrary(yardstick) # roc_curve helper\nlibrary(parsnip) # train logistic regression models\n# Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00212/\nverterbral <- read.table('data/column_2C.dat', header = FALSE, sep = ' ')\ncolnames(verterbral) <- c('pelvic_incidence', \n             'pelvic_tilt', \n             'lumbar_lordosis_angle', \n             'sacral_slope', 'pelvic_radius', \n             'degree_spondylolisthesis', 'abnormality') \nverterbral <- verterbral %>%\n select(abnormality, everything()) %>%\n mutate(id = row_number()) %>%\n mutate(abnormality = factor(if_else(abnormality == 'AB', 'yes', 'no'), \n                             levels = c('yes', 'no')))"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#split-data",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#split-data",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Split data",
    "text": "Split data\n…and split it in training (70%) and test set (30%).\n\nset.seed(992)\ntraining_ids <- verterbral %>% sample_frac(0.7) %>% pull(id)\nvert_training <- verterbral %>% \n filter(id %in% training_ids)\nvert_test <- verterbral %>% \n filter(!id %in% training_ids)"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#train-models",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#train-models",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Train models",
    "text": "Train models\n\nlogistic_model_one <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence, data = vert_training)\nlogistic_model_two <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt, data = vert_training)\nlogistic_model_three <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope, data = vert_training)\nlogistic_model_four <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius, data = vert_training)\nlogistic_model_five <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius + lumbar_lordosis_angle, data = vert_training)\nlogistic_model_all <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ ., data = vert_training[,-ncol(vert_training)])\ncheck_pred <- vert_test %>%\n select(id) %>%\n mutate( pred_logistic_one = predict(logistic_model_one, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_two = predict(logistic_model_two, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_three = predict(logistic_model_three, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_four = predict(logistic_model_four, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_five = predict(logistic_model_five, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_all = predict(logistic_model_all, vert_test, type = 'prob')$.pred_yes\n     ) %>%\n left_join(verterbral %>% select(id, abnormality), by = 'id')"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#plot-raw-outcome",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#plot-raw-outcome",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Plot raw outcome",
    "text": "Plot raw outcome\n\ncheck_pred %>% glimpse()\n\nRows: 93\nColumns: 8\n$ id                  <int> 1, 3, 6, 9, 10, 11, 12, 13, 20, 22, 27, 43, 44, 48…\n$ pred_logistic_one   <dbl> 0.7499979, 0.8028267, 0.4747471, 0.5213208, 0.4283…\n$ pred_logistic_two   <dbl> 0.7940143, 0.8233949, 0.5251698, 0.5516503, 0.3831…\n$ pred_logistic_three <dbl> 0.7939271, 0.8233653, 0.5240107, 0.5505372, 0.3813…\n$ pred_logistic_four  <dbl> 0.9301525, 0.9029361, 0.4239115, 0.5079236, 0.8360…\n$ pred_logistic_five  <dbl> 0.9035941, 0.8837432, 0.3236233, 0.5683134, 0.9143…\n$ pred_logistic_all   <dbl> 0.7531384, 0.2486521, 0.4043606, 0.8549908, 0.9448…\n$ abnormality         <fct> yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, …\n\n\nFor each observation of the test set, the models retrieve a probability. This value represents how likely that observation belongs to the label abnormality == yes[1].\nProbability is not a particular output format of logistic regressions models [2], but a standard way of many models. For instance, models based on tree decisions, such as gradient boosting [3] or random forest, retrieve probabilities as output.\nTo make it simple, for now, we will use only the predicted values (pred_logistic_all) from the trained model that used all the explanatory variables.\nSince we have all the probabilities values retrieved by the model in the variable pred_logistic_all, we can explore the distribution of the model’s outcome. To do this, there are two common ways: boxplot and density plots. For the scope of this post, we are going to use the latter. Besides, since our observations are defined by two label options (survival == ‘yes’, survival = ‘no’), we are going to plot two different distributions, one for each label:\n\ncheck_pred %>% \n ggplot(aes(pred_logistic_all)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  theme_bw() +\n  scale_fill_viridis_d()\n\n\n\n\nWe can extract some ideas from the above plot:\n\nSince this value represents the probability of an observation to belong to abnormality = 'yes', it makes sense to find observations whose real label is ‘yes’ with high probability. On the other way around, we expect to find observations whose real label is abnormality = 'no' with low probability. Though this is what we expect, this is not always the case, since we find also observations whose probability of belonging to abnormality = 'yes' is quite low, even though, its real label is yes.\nThere is a twilight zone, where we have observations from both labels levels that have “inaccurate” probabilities.\nWe can somehow see how well a model performed based on the overlapping of these two distributions.\nA perfect model would retrieve both distributions with no overlapping.\n\nSince these models do not retrieve directly the label of the response variable. A threshold to discretize a continuous probability is required to transform the probability into a label. This is a difficult part, because no matter where you define the threshold, we face a trade-off between the percentage of False Positives (FP) and False Negatives (FN). Besides, there is not a clear rule for it, and the results can be pretty arbitrary.\nAnother problem arises: if the selection of the threshold is arbitrary, how do we compare different models? Here it is where the ROC curves come out!\nROC curves try to overcome this issue, taking into account all the possible scenarios given multiple thresholds. This allows us to estimate the performance of our model independently of the threshold you take."
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#how-to-create-a-roc-curve",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#how-to-create-a-roc-curve",
    "title": "An introduction to ROC curves with animated examples",
    "section": "How to create a ROC curve?",
    "text": "How to create a ROC curve?\nTo create a ROC curve, the starting point is precisely the same information we used to display the density plot: a column with predicted probabilities and another with the real labels. Each row is an observation of the test set.\nOnce we have this information, we define as many thresholds [4] as observations found in the test set (plus Inf and -Inf). These values are defined by the probability of each observation.\nFurthermore, for each threshold value, all the probabilities above it will be identified as abnormality = yes and we count the number of True Positive (TP), True Negative (TN), but also, those observations predicted as abnormality = yes but actually are no (False Positive (FP)) and those predicted as no but actually are yes (False Negative (FN)).\nFinally, we need this information to calculate the values that will make up the ROC curve axis:\n\nSensitivity (also known as True positive rate). This metric reflects the number of positives in the test dataset that are correctly identified.\nSpecificity (also known as True negative rate). This metric measures the number of negatives in the test dataset that are correctly identified.\n\nIn both cases, a result of 1 is considered perfect.\nTo facilitate this, there are multiple packages in R to calculate the ROC curve. For this case, I am going to use the function roc_curve from the package yardstick which I recommend.\nCheck the output of the function roc_curve:\n\n# we just need to specify the column with the labels (abnormality) and the predicted probabilities (pred_logistic_all)\nroc_logistic <- check_pred %>% roc_curve(abnormality, pred_logistic_all)\nroc_logistic %>% head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf           0                1\n2     0.0179      0                1\n3     0.0292      0.0294           1\n4     0.0369      0.0588           1\n5     0.0372      0.0882           1\n6     0.0493      0.118            1\n\n\nFinally, for the visualization, we only need to modify the specificity variable as 1 - specificity:\n\nroc_logistic %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line() +\n geom_abline(linetype = 3) +\n theme_bw()"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#animated-roc-curve",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#animated-roc-curve",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Animated ROC curve",
    "text": "Animated ROC curve\nTo build some intuition, we can see how to build the ROC curve while we define thresholds values in the density plot:\n\na <- check_pred %>% \n ggplot() +\n  geom_density(aes(x = pred_logistic_all, fill = abnormality), alpha = 0.5) +\n  geom_vline(data = roc_logistic %>% filter( .threshold != Inf) %>% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) +\n  transition_reveal(.threshold) +\n  theme_bw()\nb <- roc_logistic %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line() +\n  geom_point(colour = 'red', size = 3) +\n  transition_reveal(sensitivity) +\n geom_abline(linetype = 3) +\n theme_bw()\n\n# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition\na_gif <- animate(a, width = 440, height = 440)\nb_gif <- animate(b, width = 440, height = 440)\n\nnew_gif <- image_append(c(a_gif[1], b_gif[1]))\nfor(i in 2:100){\n combined <- image_append(c(a_gif[i], b_gif[i]))\n new_gif <- c(new_gif, combined)\n}\nnew_gif"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#comparing-six-models",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#comparing-six-models",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Comparing six models",
    "text": "Comparing six models\nAt the beginning of this post, we trained five models, each one with a different number of explanatory variables: one, two, three, four, five, and six.\nWe can easily display their probability distribution:\n\ncomparison_six <- check_pred %>%\n pivot_longer(starts_with('pred'), names_to = 'model', values_to = 'prob') %>%\n mutate(model = fct_inorder(as.factor(model)))\ncomparison_six %>%\n ggplot(aes(prob)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  theme_bw() +\n  scale_fill_viridis_d() +\n facet_wrap(~ model)\n\n\n\n\nIn the example above, we observe that the overlapping between both distributions decreases as we increase the number of explanatory variables. In other words, since we increase the amount of useful information to discriminate between the two labels (yes, no), the predictive power of the model improves.\nBesides, we can plot their ROC curves:\n\ncomparison_six %>%\n group_by(model) %>%\n roc_curve(abnormality, prob) %>%\n  ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line(aes(color = model)) +\n geom_abline(linetype = 3) +\n theme_bw()"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#animated-comparison",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#animated-comparison",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Animated comparison",
    "text": "Animated comparison\nFinally, we can replicate the previous code and compare the six models:\n\nroc_comparison <- comparison_six %>%\n group_by(model) %>%\n roc_curve(abnormality, prob) %>% ungroup()\na <- comparison_six %>%\n ggplot(aes(prob)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  geom_vline(data = roc_comparison %>% filter(.threshold != Inf) %>% filter(.threshold != -Inf),\n        aes(xintercept = .threshold, group = .threshold)) +\n  theme_bw() +\n  scale_fill_viridis_d() +\n  transition_reveal(.threshold) +\n  facet_wrap(~ model)\nb <- roc_comparison %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity, group = model)) +\n  geom_line(aes(color = model)) +\n  geom_point(colour = 'red', size = 3) +\n  transition_reveal(sensitivity) +\n  geom_abline(linetype = 3) +\n  theme_bw()\n# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition\na_gif <- animate(a, width = 440, height = 440)\nb_gif <- animate(b, width = 440, height = 440)\n\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n\nnew_gif <- image_append(c(a_gif[1], b_gif[1]))\nfor(i in 2:100){\n combined <- image_append(c(a_gif[i], b_gif[i]))\n new_gif <- c(new_gif, combined)\n}\nnew_gif"
  },
  {
    "objectID": "posts/roc-curves/roc-curves-animated-examples.html#notes",
    "href": "posts/roc-curves/roc-curves-animated-examples.html#notes",
    "title": "An introduction to ROC curves with animated examples",
    "section": "Notes",
    "text": "Notes\n[1] The package yardstick as many other packages use the first level of the response variable factor as the “event”. Therefore, the probability output determines how likely an observation belongs to the first level of the factor of the response variable. This behavior can be changed in the yardstick package global options.\n[2] The default output has a logit scale and needs to be transformed first to a probability value. This can be done automatically if we specify in the function predict the argument type as prob.\n[3] Apart from tree decisions, linear models can be also used in gradient boosting.\n[4] Because of this reason, ROC curves might not be appropriate to evaluate the performance of models on small test sets."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "",
    "text": "In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance. Furthermore, we will explore the advantage of using a Bayesian model when we want to estimate how likely is our prediction. Finally, we will briefly discuss why there are some predicted values more probable than others.\n\n\nFirst, we download this data from Kaggle. This dataset includes 306 patients from a study of patients that had undergone a surgical operation on breast cancer. The table consists of three explanatory variables:\n\nAge of patient during surgical operation (age)\nYear when the operation was made (operation_year)\nNumber of positive axillary nodes detected (nodes)\n\nFurthermore, there is a column (survival) that indicates whether the patient survived at least 5 years after the operation.\n\nlibrary(tidyverse)\nlibrary(patchwork) # merge plots\nlibrary(ggridges) # ridges plot\nlibrary(glue) # paste plot labels\nlibrary(yardstick) # helper roc curves and auc\nlibrary(rstanarm) # bayesian model\nlibrary(bayestestR) # helper for the bayesian model\nlibrary(broom) # make tidy\n# Source: https://www.kaggle.com/gilsousa/habermans-survival-data-set\nhaberman <- read_csv('data/haberman.csv', col_names = c('age', \n                            'operation_year', \n                            'nodes', \n                            'survival'))\nhaberman <- haberman %>%\n mutate(survival = factor(if_else(survival == 1, 'Yes', 'No'))) %>%\n mutate(operation_year = factor(operation_year)) %>%\n mutate(id = as.character(row_number())) %>% \n select(id, everything())\n\n\n\n\nSince the dataset has 3 explanatory variables, let’s plot the distribution of each one of them with the response variable survival:\n\np1 <- haberman %>%\n ggplot(aes(age)) +\n geom_density(aes(fill = survival), color = 'black', alpha = 0.4) +\n theme_bw()\np2 <- haberman %>%\n ggplot(aes(nodes)) +\n geom_density(aes(fill = survival), color = 'black', alpha = 0.4) +\n theme_bw()\np3 <- haberman %>%\n group_by(operation_year, survival) %>%\n summarise(n = n()) %>%\n mutate(perc = 100*(n / sum(n))) %>%\n ggplot(aes(operation_year, perc)) +\n geom_col(aes(fill = survival), color = 'black') +\n theme_bw() +\n labs(y = 'Percentage (%)')\n\n`summarise()` has grouped output by 'operation_year'. You can override using\nthe `.groups` argument.\n\np1 + p2 + p3 + patchwork::plot_layout(nrow = 3)\n\n\n\n\nAge and number of nodes seem to have a reasonable distribution but surprisingly, patient survival does not increase along the operation year. In theory, the patient survival of most cancer types has increased dramatically over the years. Therefore, it seems reasonable to find a similar pattern in this dataset. The interval of time (1958-1969) seems long enough and happened during a period of major progress in clinical therapies.\nA plausible explanation is an underlying effect of, at least, one remaining variable. Let’s observe the distribution of the variable age of patient over the years:\n\nhaberman %>%\n ggplot(aes(age, operation_year)) +\n geom_density_ridges(aes(fill = operation_year), show.legend = FALSE) +\n theme_bw()\n\n\n\nhaberman %>%\n ggplot(aes(operation_year, age)) +\n geom_boxplot(aes(fill = operation_year), show.legend = FALSE) +\n theme_bw()\n\n\n\n\nThere seem to be differences over the years. In this post, further analysis to control for this effect is out of scope, but a more exhaustive analysis of this dataset should be aware of it."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#split-data",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#split-data",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Split data",
    "text": "Split data\nFirst, we will split the available dataset haberman into two sets, a training (70%) and a test (30%).\n\nset.seed(991)\ntraining_ids <- haberman %>% sample_frac(0.7) %>% pull(id)\nhab_training <- haberman %>% \n filter(id %in% training_ids) %>% \n mutate(operation_year = as.integer(operation_year))\nhab_test <- haberman %>% \n filter(!id %in% training_ids) %>% \n mutate(operation_year = as.integer(operation_year))\n\nWe will train independently both models with the training set and predict the labels of the response variable (“survive”, “no survive”) in the test dataset. These predicted labels will be useful to compare both models in terms of performance and further aspects."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#training---logistic-regression",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#training---logistic-regression",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Training - logistic regression",
    "text": "Training - logistic regression\nIn R, we just need to use the glm function and specify the argument family = binomial:\n\nlogistic_model <- glm(survival ~ age + nodes + operation_year, family = 'binomial', \n           data = hab_training)"
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#training---bayesian-logistic-regression",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#training---bayesian-logistic-regression",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Training - Bayesian logistic regression",
    "text": "Training - Bayesian logistic regression\nThanks to the package rstanarm that provides an elegant interface to stan, we can keep almost the same syntax used before. In this case, we use the function stan_glm:\n\nbayesian_model <- rstanarm::stan_glm(survival ~ age + nodes + operation_year, \n                   family = 'binomial',\n                   data = hab_training,\n                   prior = normal())\n\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 1:                0.113 seconds (Sampling)\nChain 1:                0.249 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.154 seconds (Warm-up)\nChain 2:                0.117 seconds (Sampling)\nChain 2:                0.271 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.156 seconds (Warm-up)\nChain 3:                0.115 seconds (Sampling)\nChain 3:                0.271 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.117 seconds (Sampling)\nChain 4:                0.253 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#performance",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#performance",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Performance",
    "text": "Performance\nOnce we trained both models, we are going to compare their performance with the test set (split at the beginning of the post). To that end, we calculate the ROC curve and the Area Under the Curve (AUC) of each model:\n\npred_logistic <- predict(logistic_model, hab_test, type = 'response')\npred_bayesian <- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %>% \n as_tibble() %>%\n map_dbl(~ map_estimate(.x) )\n\nInstead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\n\ncheck_pred <- hab_test %>%\n select(id) %>%\n mutate(pred_surv_no_log = pred_logistic,\n     pred_surv_no_bay = pred_bayesian) %>%\n left_join(haberman %>% select(id, survival), by = 'id')\nroc_logistic <- check_pred %>% roc_curve(survival, pred_surv_no_log) %>% mutate(model = 'logistic')\nroc_bayesian <- check_pred %>% roc_curve(survival, pred_surv_no_bay) %>% mutate(model = 'bayesian')\nauc_logistic <- check_pred %>% roc_auc(survival, pred_surv_no_log) %>% pull(.estimate) %>% round(3)\nauc_bayesian <- check_pred %>% roc_auc(survival, pred_surv_no_bay) %>% pull(.estimate) %>% round(3)\nroc_both <- roc_logistic %>% bind_rows(roc_bayesian)\nroc_both %>%\n ggplot(aes((1-specificity), sensitivity)) +\n  geom_line(aes(color = model), size = 1) +\n  theme_bw() +\n geom_abline(linetype = 3) +\n  labs(title = 'Comparison performance logistic and Bayesian model',\n     subtitle = glue('AUC (logistic) = {auc_logistic} - AUC (Bayesian) = {auc_bayesian}'))\n\n\n\n\nBoth models demonstrate similar performance. If we would have to decide, at this step of the analysis, one of them (logistic or Bayesian), there would not be a reason to choose one or the other. Probably, the logistic one, since it may sounds more familiar. But this might change when the uncertainty idea comes up!"
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#uncertainty",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#uncertainty",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Uncertainty",
    "text": "Uncertainty\nFirst, we are going to explore the outcomes of the test set provided by the logistic model. These values represent the probability [2] of each instance of being labeled as “No survive” five years after the operation:\n\npred_logistic <- predict(logistic_model, hab_test, type = 'response')\np1 <- pred_logistic %>%\n enframe() %>%\n ggplot(aes(value)) +\n geom_density(fill = 'steelblue', alpha = 0.5) +\n theme_bw() +\n labs(x = 'Probability', y = 'Density')\np2 <- pred_logistic %>%\n enframe() %>%\n ggplot(aes(value)) +\n geom_histogram(fill = 'yellow', alpha = 0.5, color = 'black', binwidth = 0.05) +\n theme_bw() +\n labs(x = 'Probability', y = 'Density')\np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\nEach probability value represents a single observation. To convert the predicted probability to labels, the user needs to specify a threshold where every value above the threshold is defined as “No survive”, otherwise “survive”. Most of the cases, this creates problematic scenarios where two observations can be equally labeled in spite of having distinct probabilities (e.g. 0.6 and 0.95).\nBy contrast, for each one of observations in the test set, the Bayesian model does not provide a single probability value but a posterior distribution. We can represent the posterior distributions from the 92 observations (test set) with a boxplot, for instance:\n\nplot_uncertainty <- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %>% \n as_tibble() %>%\n pivot_longer(everything(), names_to = 'rank_obs', values_to = 'pred_surv_n_bay')\n\nInstead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\n\nplot_uncertainty %>%\n ggplot(aes(rank_obs, pred_surv_n_bay)) +\n geom_boxplot() +\n theme_bw() +\n labs(x = 'Test set - Observation', y = 'Probability (survival == \"No survive\")')\n\n\n\n\nAnother way to check the dispersion of the predicted outcome is with a ridge plot, that is especially useful when the number of samples is low. So, let’s pick only two observations:\n\nplot_uncertainty %>%\n # if you want to reproduce this code, just change the character '1' or '5' for any other.\n filter(rank_obs %in% c('1', '5')) %>%\n ggplot(aes(pred_surv_n_bay, rank_obs)) +\n  geom_density_ridges(aes(fill = rank_obs), alpha = 0.6) +\n  theme_bw() +\n  labs(y = 'Test set - Observation', x = 'Probability (survival == \"No survive\")')\n\nPicking joint bandwidth of 0.0102\n\n\n\n\n\nIn the above plot, we observe that both distributions have a “peak” (known as MAP [1]) above 0.5, therefore the predicted label would be, in both cases, ‘no survive’. But, are these two predictions equally certain? Well, we notice, at least, two things:\n\nBoth MAPs have different values (sample 1 - 0.9, sample 5 - 7.2).\nObservation number 5 has a flatter curve in comparison with the number 1.\n\nIn both cases, observation 5 reflects a higher uncertainty regarding its predicted label in comparison with observation 1. Should we make the same clinical decisions in both cases? Would this information be valuable in a clinical environment…? Probably yes, but first, we should find a way to measure it."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#measuring-uncertainty-3",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#measuring-uncertainty-3",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Measuring uncertainty [3]",
    "text": "Measuring uncertainty [3]\nA handy option is to use the standard deviation (sd) of the distribution, so we can estimate one value for each observation. With this in mind, we can plot the distribution of the sd from the 92 observations of the test dataset:\n\nstd_dev_tbl <- plot_uncertainty %>%\n group_by(rank_obs) %>%\n summarise(std_dev = sd(pred_surv_n_bay)) %>%\n ungroup()\nstd_dev_tbl %>% \n ggplot(aes(std_dev)) +\n geom_density(fill = 'steelblue', alpha = 0.5) +\n theme_bw() +\n labs(x = 'Standard deviation (sd)',\n    y = 'Density') \n\n\n\n\nMost of observations have a standard deviation of around 0.04. There are a few extreme values in the interval 0.08-0.10. In short, this plot shows that there are some observations whose sd is twice as high as others.\nWe can filter and select observations based on the dispersion of its posterior distribution. For instance, we can split the test set of 92 observations in percentiles using the sd and plot the 1st (lowest sd) and 10th percentile (highest sd). In this way, it allows us to compare those observations with the highest and lowest standard deviation:\n\ntop_sd <- std_dev_tbl %>% \n mutate(tile = ntile(std_dev, 10)) %>%\n filter(tile == 1 | tile == 10)\nplot_uncertainty %>%\n # left_join(std_dev, by = 'rank_obs') %>%\n filter(rank_obs %in% top_sd$rank_obs) %>%\n ggplot(aes(pred_surv_n_bay, rank_obs)) +\n geom_density_ridges(aes(fill = rank_obs), alpha = 0.6, show.legend = FALSE) +\n theme_bw() +\n labs(y = 'Test set - Observation', x = 'Probability (survival == \"No survive\")')\n\n\n\n\nIn the above plot, we easily identify to which group each observation belongs to. Independently of the predicted labels, should their predictions be considered equally likely? If the final user of the model just receives a categorical outcome, he/she is definitely skipping some valuable information since some predictions look more unlikely than others. As an alternative, predictions could be grouped into categories and neglect those with a high dispersion or make it clear than further support should be required.\nIn this post, we have measured the uncertainty of observations and identifying those samples with high uncertainty. But, we have not talked yet about what is the origin of it."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#why-some-predictions-are-more-unlikely-than-others",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#why-some-predictions-are-more-unlikely-than-others",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Why some predictions are more unlikely than others?",
    "text": "Why some predictions are more unlikely than others?\nIn other words, why our model has more doubts about a sample than others? I find two possible explanations:\n\nThe sample is mislabeled.\nGroup variability.\n\nThe first one is difficult to address but we can explore the group variability. Since we have three continuous explanatory variable, we can easily do a PCA with the function prcomp:\n\npca_tbl <- hab_test %>%\n # take only numeric columns\n select_if(is.numeric) %>%\n # Important - we need to scale and center each variable before PCA\n prcomp(scale = TRUE, center = TRUE) %>%\n tidy() %>%\n mutate(row = as.character(row)) %>%\n pivot_wider(id_cols = row, values_from = value, names_from = PC, names_prefix = 'PC') %>%\n left_join(top_sd %>% select(-std_dev), by = c('row' = 'rank_obs')) %>%\n mutate(tile = ifelse(is.na(tile), 'ok', tile)) %>%\n left_join(hab_test %>% select(survival) %>% mutate(row = as.character(row_number())), \n      by = 'row')\npca_tbl %>%\n ggplot(aes(PC1, PC2)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nAs we have observations in two categories, let’s split them into two plots:\nSince we have observations defined into two categories (survival = Yes, survival = No), let’s lay out the plot into two different:\n\npca_tbl %>%\n ggplot(aes(PC1, PC2)) +\n  geom_point() +\n  theme_bw() +\n  facet_grid(~ survival)\n\n\n\n\nFurthermore, we are going to highlight those observations that belong to the highest and lowest sd groups:\n\npca_tbl %>%\n mutate(tile = case_when(\n  tile == '1' ~ 'lowest sd',\n  tile == '10' ~ 'highest sd',\n  tile == 'ok' ~ 'ok'\n )) %>%\n ggplot(aes(PC1, PC2)) +\n  geom_point(aes(fill = tile), color = 'black', shape = 21, size = 2) +\n  theme_bw() +\n  facet_grid(~ survival) +\n  labs(fill = 'Category')\n\n\n\n\nOn the one hand, “lowest sd” group observations are centrally located in the plot. This reflects a tendency of these samples to have similar features values with observations belonging to their own label. On the other hand, “highest sd” group points tend to be dispersed from the rest, all over the components. It makes sense since the uncertainty to predict these points come from the fact that their own feature values are different from points on the same category.\nSurprisingly, there is a red point on the left panel whose location is centric respect to the rest of the values. This perhaps arises the disadvantage of reducing a probability distribution to a point-estimate (standard deviation). The dispersion estimation might have not be accurate enough and further ways of measuring might be needed."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#conclusion",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#conclusion",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Conclusion",
    "text": "Conclusion\nAs humans, we make decisions based on uncertainty, even though we are not aware of it. If the weather forecasters show 10% of raining on the weekend, we will probably make a plan to go to the mountain. With 90% we may rethink about it…When we are talking with someone about a delicate topic, we pick the words based on the uncertainty of his/her predicted response: words with a broad meaning and therefore ambiguous might not be chosen, due to the high uncertainty. Therefore, if we constantly map our reality and act through the constant evaluation of uncertainty, why should we believe in predictions from machines without a shadow of doubt?\nMy personal view is that the measurement of uncertainty will end up being essential. Especially, for every decision process supported by a machine in a clinical environment. In that way, I find Bayesian models a nice fit for many of the challenges of tomorrow."
  },
  {
    "objectID": "posts/uncertainty/uncertainty-bayesian-models.html#notes",
    "href": "posts/uncertainty/uncertainty-bayesian-models.html#notes",
    "title": "An introduction to uncertainty with Bayesian models",
    "section": "Notes",
    "text": "Notes\n[1] To calculate the roc curves of the Bayesian model’s predictions, a single probability value for each observation is required. There are multiple ways to estimate it, such as mean, median, and MAP (Highest Maximum A Posteriori). In this case, I chose the latest because it provided the highest performance.\n[2] The function predict retrieves outcomes as probabilities because we specified type = response as argument. Otherwise, the default output would be as logit.\n[3] I am interested to know more ways to estimate the “uncertainty” of a prediction. Please if you have any reference or idea, let me know! ;P"
  },
  {
    "objectID": "posts/genomics-england-panelapp/scrapping-genomics-england-panelapp.html",
    "href": "posts/genomics-england-panelapp/scrapping-genomics-england-panelapp.html",
    "title": "Extracting gene panels from the Genomics England Panelapp",
    "section": "",
    "text": "The Genomics England PanelApp provides panels of genes related to human disorders manually curated by healthcare experts. From a clinical and research perspective, this is a remarkable resource. At the time of writing this post, over 320 panels have been published.\nUnfortunately, you can only download the panels manually one at a time or through an API that retrieves the information as a JSON file.\nAlternatively, below you can find a script in R to extract all the panels from the website and merge them into a single dataset. Please note the following points before using the script:\n\nI only consider genes labeled as “Expert Review Green” defined as “gene-disease association with a high level of evidence” and exclude STRs and CNVs entities. More information on the criteria used can be found on the main page (heading: Understanding gene classifications in a version 1+ gene panel.\nThe script selects only a subset of columns from the total available. Be careful, the script will download more than 320 files automatically (on my laptop, the execution process is ~7 min).\nThe script is ready to run on the Linux system.\n\nAs the script is based on the current website structure, any changes could break the code. Please let me know if this happens. I will try to code an updated version of the code.\n\nlibrary(rvest)\nlibrary(purrr)\nlibrary(tidyverse)\nwebsite <- \"https://panelapp.genomicsengland.co.uk/panels/\"\npage <- read_html(website)\nc_ref <- page %>%\n  html_nodes(\"a\") %>% # find all links\n  html_attr(\"href\")\ndf_ref <- tibble(ref = c_ref, id = NA) %>%\n  filter(str_detect(ref, 'download')) %>%\n  mutate(ref = str_remove(ref, '/panels/')) %>%\n  mutate(id = ref) %>%\n  mutate(id = str_remove(id, '/download/01234/'))\n# Linux command - if you are using Windows, please make sure that you create a new folder with the name 'gene_panel'\n# and remove the \"system('mkdir gene_panel')\" line\nsystem('mkdir gene_panel')\nsetwd('gene_panel')\nwalk2(df_ref$ref, df_ref$id, function(a, b)\n  download.file(url = paste0(website, a), destfile = paste0('gene_panel_', b))\n)\nfiles_panel <- list.files()\npanel_total <- files_panel %>% map_dfr(~ read_tsv(.x) %>% \n                                         select(`Entity Name`, `Entity type`, `Gene Symbol`, `Sources(; separated)`, \n                                                Level4, Phenotypes) %>% \n                                         mutate(source = .x) )\n# Filtering out genes with a evidence level (red - amber)\npanel_total <- panel_total %>%\n  rename(entity_name = `Entity Name`,\n         entity_type = `Entity type`,\n         gene = `Gene Symbol`,\n         sources = `Sources(; separated)`) %>%\n  filter(entity_type == 'gene') %>%  # optional - we can include regions in our analysis\n  filter(str_detect(sources, 'Expert Review Green')) %>%\n  select(gene, Level4, -sources, source, Phenotypes)\nsetwd('..')\nwrite_tsv(panel_total, 'panel_genes.tsv')"
  },
  {
    "objectID": "posts/cancer-genes-pubmed/cancer-gene-association-pubmed.html",
    "href": "posts/cancer-genes-pubmed/cancer-gene-association-pubmed.html",
    "title": "How many genes have been associated with cancer in PubMed?",
    "section": "",
    "text": "In the biomedical literature, it is common to find sentences like:\n“Besides, the gene [gene symbol] has been associated with [type of cancer(s)] [References]”\nThe structure of these sentences can change from article to article, but the underlying idea and goal are the same. I will try to summarise it in the following sentence:\n“Hello reader/editor/reviewer, I was studying [any field], and I found this gene. I think it is a relevant/remarkable finding because it has been associated with cancer [references]. Therefore, it supports my hypothesis about the biological relevance of the gene in my field. Please, publish it.”\nThis approach is valid and logical as long as the association gene <-> cancer has been well-described and validated by different experiments and research teams. Unfortunately, some of these associations will be just spurious and no well-supported.\nTo explore this problem, we will count the number of articles in PubMed associating cancer with each one of the 19,205 protein-coding genes in the human genome.\nTo do so, we will write a simple code in R that will make a query for each gene to PubMed using the fantastic rentrez package.\nThe script has two simple steps:\n\nDownload the official list of protein-coding gene symbols from HUGO.\nFor each gene, make the following query “gene_symbol[Title/Abstract] AND cancer[Title/Abstract]” in PubMed (1-2).\n\nYou can find the code below:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(rentrez)\ngene_symbols <- read_tsv('http://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/locus_types/gene_with_protein_product.txt') %>% \n  pull(symbol)\n# Careful: it takes long to make all the queries\nquery_pubmed <- function(input_gene) {\n  \n  print(input_gene)\n  Sys.sleep(0.3)\n  query_tmp <- entrez_search(db =\"pubmed\", \n                             term = paste(paste0(input_gene, '[Title/Abstract]'),' AND ', 'cancer[Title/Abstract]'), \n                             retmax = 600)\n  \n  tibble('gene' = input_gene, 'n_hits' = length(query_tmp[['ids']]))\n}\nresult_genes <- gene_symbols %>% map_dfr(~ query_pubmed(.x))\n\n\nresult_genes %>%\n  ggplot(aes(n_hits)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(x = 'Nº articles', y = 'Nº genes')\n\n\n\nresult_genes %>%\n  mutate(category = case_when(\n    n_hits == 0 ~ '0 articles',\n    n_hits >= 1 & n_hits <= 5 ~ '1-5 articles',\n    TRUE ~ '>5 articles'\n  )) %>% \n  count(category) %>%\n  mutate(perc = n / sum(n)) %>%\n  ggplot(aes(reorder(category,perc), perc)) +\n    geom_col(aes(fill = category), color = 'black') +\n  scale_y_continuous(label = percent, limits = c(0, 1)) +\n  geom_label(aes(label = paste0(round(perc, 2)*100, '%'))) +\n  labs(fill = 'Category', x = 'Category', y = 'Percentage') +\n  theme_minimal()\n\n\n\n\nAs you can see in the plot, 41% of the genes have been associated with cancer in more than five articles, 36% in 1-5 articles, and only 23% of the genes with no publications.\nIf I choose a random protein-coding from the human genome and do a query in PubMed, it is more likely (77%) to find at least one article than none.\nThis data reflects how easy it is to find articles associating cancer with most of the genes. Therefore, when a reader finds this kind of argument [my gene is important -> gene + cancer + references] should take it with a grain of salt.\nAn interesting point is the reasons behind these numbers. From a biological perspective, it is difficult to assume the relevance in cancer of most of the human genome even though cancer is a group including many different kinds of diseases with their subgroups.\nIn the following points, I describe some of the reasons that might explain these numbers:\n\nIn most cases, a tumor is produced by the disruption of multiple genes simultaneously. Structural variants or chromosomal aberrations can map a considerable portion of the genome and disrupt many genes. This scenario makes it difficult to identify the driver mutations from those mutations (passengers) with no relevance and, therefore, finding the causal gene(s). Researchers have an important incentive: publish. Therefore, it makes sense that some researchers “orientate” their studies and results to the cancer field because it is a way to give more “weight” to their research even though, in some cases, the evidence for it is scarce.\nThe overproduction of scientific studies in specific areas of knowledge has already been described. For instance, in a comment published in 2021, the authors find that only 22% of gene-related publications were related to 1% of genes. Also, they find “new yearly publications focusing on a given gene is linearly proportional to the size of previous literature on it”.\n\nIt is reasonable to think that a similar scenario happens with many research published trying to link their analysis with any aspect of cancer though the evidence is limit.\nTo clarify, this is by no means a way to discredit researchers with work related to cancer. It is a way to make people aware of the problematic aspect of finding articles in PubMed describing the gene A associated with cancer and using them as evidence without further analysis.\nSome ideas for a future version\n\nThere have been multiple naming conventions to identify genes. A paper describing a gene with a synonymous symbol instead of the official gene will not be reported in our script. Luckily, HUGO reports the list of synonymous symbols along with the official one. Therefore, it would be easy to adapt our script with new queries and merge the number of hits.\nSome gene symbols can be confounded with composited names. For instance, the query “A1BG[Title/Abstract] AND cancer[Title/Abstract]” retrieved 22 hits, and one of them was describing the A1BG-AS1 lncRNA. An interesting idea would be to reanalyze the data but change cancer by each of the cancer types.\n\nNotes\n\nWe can run the queries in parallel, adding the tag “future_” to the function “map_dfr” thanks to the package furrr. Note the API has a limit of simultaneous queries per IP.\nI set a limit of 600 articles retrieved to avoid the exceeded limit error of the API."
  },
  {
    "objectID": "posts/human-genetics-drugs/human-genetics-drug-discovery.html",
    "href": "posts/human-genetics-drugs/human-genetics-drug-discovery.html",
    "title": "Human genetics as a tool for drug discovery",
    "section": "",
    "text": "For children with a rare disease, an accurate diagnosis is crucial to provide advice, possible therapies and assess the potential risk for family members in future generations. Public initiatives such as the International Rare Diseases Research Consortium (IRDiRC) set the goal for 2017-2027 to “enable all people living with a rare disease to receive an accurate diagnosis, care, and available therapy soon after seeking medical care” (1).\nDespite significant efforts in the field of drug development, the reality is discouraging. Drug candidates that move from phase 1 clinical trials to approval and launch remain at around 10%. This rate is even lower in some specific fields, such as oncology (3.4%).\nAt the same time, large-scale biomedical databases, such as gnomAD and the UK Biobank, offer the possibility to assess the effect of variants in humans and their association with multiple conditions. In addition, the recruitment of population-based cohorts considered disease-agnostic increases the number of hypotheses that can be tested. In particular, the high scientific value of the UK Biobank is worth mentioning, as it not only releases genomic data but also rich phenotype characterization alongside other data sources (273).\nBoth mutations and drugs share the same mechanism: they disrupt the normal functioning of the human body. How they do so may differ, but it seems plausible to hypothesize that, in some cases, the phenotypic consequences of a loss-of-function SNVs or deletions are similar to the pharmacological effect of an inhibitor drug.\nThere is no doubt that this view poses some problems. As mentioned above, the heterogeneity and particularities in genetics are enormous even for monogenic diseases. Elements such as the genotype of the variant or discrepancies between the predicted effect and the actual consequences can make the use of this approach challenging. For instance, it is already known that LoF mutations may not decrease protein or even mRNA levels.\nIn spite of these drawbacks, human genetics seems to be a great tool for the identification of drugs with therapeutic effects.\nThere is evidence to support this assumption. For instance, gastrointestinal adverse events observed in clinical trials of DGAT1 inhibitors could have been predicted based on the causal relationship between rare and highly penetrant variants of DGAT1 and congenital diarrheal disorder (3).\nAnother well-known example is the association of heterozygous gain-of-function mutations in the PCSK9 gene and familial hypercholesterolemia, which leads to heart attacks or strokes relatively early in life. Strikingly, LoF variants in the PCSK9 gene have been causally associated with low levels of low-density lipoprotein cholesterol (4).\nThis human genetic evidence contributed to the technical and regulatory success of PCSK9 inhibitors, leading to the launch of evolocumab (Amgen) and alirocumab (Regeneron), and has also shown value in patient stratification in clinical trials (5).\nOther drug candidates with strong genetic evidence between disease phenotypes and functional genetic variants have been identified, such as HSD17B13 for chronic liver disease (6), TYK2 for multiple autoimmune disorders (7), NRXN1 for neuropsychiatric disease (8), ASGR1 for cardiovascular disease (9),\nThese are not isolated examples, but a general trend. Nelson et al. found that pairing genetic target indication with genetic evidence almost doubles the success rate in clinical development. These analyses were re-evaluated three years later by other researchers with data after the original publication and controlling for potential confounding factors and corroborated the same claims (10).\nHarnessing human genomic data can improve the drug discovery process, from target selection to reducing failures due to lack of efficacy or adverse effects."
  },
  {
    "objectID": "posts/human-genetics-drugs/human-genetics-drug-discovery.html#references",
    "href": "posts/human-genetics-drugs/human-genetics-drug-discovery.html#references",
    "title": "Human genetics as a tool for drug discovery",
    "section": "References",
    "text": "References\n\n.Sanders AD, Falconer E, Hills M, Spierings DCJ, Lansdorp PM. Single-cell template strand sequencing by Strand-seq enables the characterization of individual homologs. Nat Protoc. 2017;12: 1151–1176.\nSzustakowski JD, Balasubramanian S, Kvikstad E, Khalid S, Bronson PG, Sasson A, et al. Advancing human genetics research and drug discovery through exome sequencing of the UK Biobank. Nat Genet. 2021;53. doi:10.1038/s41588-021-00885-0\nHaas JT, Winter HS, Lim E, Kirby A, Blumenstiel B, DeFelice M, et al. DGAT1 mutation is linked to a congenital diarrheal disorder. J Clin Invest. 2012;122: 4680–4684.\nCohen JC, Boerwinkle E, Mosley TH Jr, Hobbs HH. Sequence variations in PCSK9, low LDL, and protection against coronary heart disease. N Engl J Med. 2006;354: 1264–1272.\nSabatine MS, Giugliano RP, Keech AC, Honarpour N, Wiviott SD, Murphy SA, et al. Evolocumab and Clinical Outcomes in Patients with Cardiovascular Disease. N Engl J Med. 2017;376: 1713–1722.\nAbul-Husn NS, Cheng X, Li AH, Xin Y, Schurmann C, Stevis P, et al. A Protein-Truncating HSD17B13 Variant and Protection from Chronic Liver Disease. N Engl J Med. 2018;378: 1096–1106.\nDendrou CA, Cortes A, Shipman L, Evans HG, Attfield KE, Jostins L, et al. Resolving TYK2 locus genotype-to-phenotype differences in autoimmunity. Sci Transl Med. 2016;8: 363ra149.\nNoh HJ, Tang R, Flannick J, O’Dushlaine C, Swofford R, Howrigan D, et al. Integrating evolutionary and regulatory information with a multispecies approach implicates genes and pathways in obsessive-compulsive disorder. Nat Commun. 2017;8: 774.\nNioi P, Sigurdsson A, Thorleifsson G, Helgason H, Agustsdottir AB, Norddahl GL, et al. Variant ASGR1 Associated with a Reduced Risk of Coronary Artery Disease. N Engl J Med. 2016;374: 2131–2141.\nKing EA, Davis JW, Degner JF. Are drug targets with genetic support twice as likely to be approved? Revised estimates of the impact of genetic support for drug mechanisms on the probability of drug approval. PLoS Genet. 2019;15: e1008489."
  }
]