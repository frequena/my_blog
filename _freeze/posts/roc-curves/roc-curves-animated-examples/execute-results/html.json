{
  "hash": "29a0559cb28feda831f3a4e56352556c",
  "result": {
    "markdown": "---\ntitle: \"An introduction to ROC curves with animated examples\"\nformat: html\neditor: visual \ntoc: true\nauthor: Francisco Requena\ndate: 6/12/2020 \nsidebar: true\nimage: images/image.png\ncategories:\n  - animation\n  - R\n  - machine-learning\n---\n\n\n\n## Overview\n\nReceiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance. Possibly, because we are used to interpreting information as single values, such as mean, median, accuracy...ROC curves are different because it represents a group of values conforming a curve. Besides, it is the most popular way to represent a model performance for a *particular dataset* where the task is a binary classification.\n\nBefore explaining where the ROC curves come from, let's focus on what is the outcome of most of the classification models. To illustrate this point, let's train a few logistic regression models with a toy dataset and use the package `parsnip` which provides a common interface to train models from many other packages.\n\n## Data\n\nFor this post, we are going to use a dataset that includes 310 patients and six explanatory variables related to biomechanical features of the vertebral column. Besides, it contains a response variable `abnormality` that defines if the patient has been diagnosed with a medical condition in the vertebral column (`yes` and `no`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(gganimate) # animated plots\nlibrary(magick) # combine two gif\nlibrary(yardstick) # roc_curve helper\nlibrary(parsnip) # train logistic regression models\n# Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00212/\nverterbral <- read.table('data/column_2C.dat', header = FALSE, sep = ' ')\ncolnames(verterbral) <- c('pelvic_incidence', \n             'pelvic_tilt', \n             'lumbar_lordosis_angle', \n             'sacral_slope', 'pelvic_radius', \n             'degree_spondylolisthesis', 'abnormality') \nverterbral <- verterbral %>%\n select(abnormality, everything()) %>%\n mutate(id = row_number()) %>%\n mutate(abnormality = factor(if_else(abnormality == 'AB', 'yes', 'no'), \n                             levels = c('yes', 'no')))\n```\n:::\n\n\n\n## Split data\n\n...and split it in training (70%) and test set (30%).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(992)\ntraining_ids <- verterbral %>% sample_frac(0.7) %>% pull(id)\nvert_training <- verterbral %>% \n filter(id %in% training_ids)\nvert_test <- verterbral %>% \n filter(!id %in% training_ids)\n```\n:::\n\n\n## Train models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_model_one <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence, data = vert_training)\nlogistic_model_two <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt, data = vert_training)\nlogistic_model_three <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope, data = vert_training)\nlogistic_model_four <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius, data = vert_training)\nlogistic_model_five <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius + lumbar_lordosis_angle, data = vert_training)\nlogistic_model_all <- logistic_reg() %>%\n set_engine(\"glm\") %>%\n set_mode(\"classification\") %>%\n fit(abnormality ~ ., data = vert_training[,-ncol(vert_training)])\ncheck_pred <- vert_test %>%\n select(id) %>%\n mutate( pred_logistic_one = predict(logistic_model_one, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_two = predict(logistic_model_two, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_three = predict(logistic_model_three, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_four = predict(logistic_model_four, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_five = predict(logistic_model_five, vert_test, type = 'prob')$.pred_yes,\n     pred_logistic_all = predict(logistic_model_all, vert_test, type = 'prob')$.pred_yes\n     ) %>%\n left_join(verterbral %>% select(id, abnormality), by = 'id')\n```\n:::\n\n\n## Plot raw outcome\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_pred %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 93\nColumns: 8\n$ id                  <int> 1, 3, 6, 9, 10, 11, 12, 13, 20, 22, 27, 43, 44, 48…\n$ pred_logistic_one   <dbl> 0.7499979, 0.8028267, 0.4747471, 0.5213208, 0.4283…\n$ pred_logistic_two   <dbl> 0.7940143, 0.8233949, 0.5251698, 0.5516503, 0.3831…\n$ pred_logistic_three <dbl> 0.7939271, 0.8233653, 0.5240107, 0.5505372, 0.3813…\n$ pred_logistic_four  <dbl> 0.9301525, 0.9029361, 0.4239115, 0.5079236, 0.8360…\n$ pred_logistic_five  <dbl> 0.9035941, 0.8837432, 0.3236233, 0.5683134, 0.9143…\n$ pred_logistic_all   <dbl> 0.7531384, 0.2486521, 0.4043606, 0.8549908, 0.9448…\n$ abnormality         <fct> yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, …\n```\n:::\n:::\n\n\nFor each observation of the test set, the models retrieve a probability. This value represents how likely that observation belongs to the label `abnormality == yes`[1]. \n\nProbability is not a particular output format of logistic regressions models [2], but a standard way of many models. For instance, models based on tree decisions, such as gradient boosting [3] or random forest, retrieve probabilities as output.\n\nTo make it simple, for now, we will use only the predicted values (`pred_logistic_all`) from the trained model that used all the explanatory variables.\n\nSince we have all the probabilities values retrieved by the model in the variable `pred_logistic_all`, we can explore the distribution of the model's outcome. To do this, there are two common ways: boxplot and density plots. For the scope of this post, we are going to use the latter. Besides, since our observations are defined by two label options (survival == 'yes', survival = 'no'), we are going to plot two different distributions, one for each label:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_pred %>% \n ggplot(aes(pred_logistic_all)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  theme_bw() +\n  scale_fill_viridis_d()\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe can extract some ideas from the above plot:\n\n- Since this value represents the probability of an observation to belong to `abnormality = 'yes'`, it makes sense to find observations whose real label is 'yes' with high probability. On the other way around, we expect to find observations whose real label is `abnormality = 'no'` with low probability. Though this is what we expect, this is not always the case, since we find also observations whose probability of belonging to `abnormality = 'yes'` is quite low, even though, its real label is `yes`.\n\n- There is a twilight zone, where we have observations from both labels levels that have \"inaccurate\" probabilities. \n\n- We can somehow see how well a model performed based on the overlapping of these two distributions.\n\n- A perfect model would retrieve both distributions with no overlapping.\n\nSince these models do not retrieve directly the label of the response variable. A threshold to discretize a continuous probability is required to transform the probability into a label. This is a difficult part, because no matter where you define the threshold, we face a trade-off between the percentage of False Positives (FP) and False Negatives (FN). Besides, there is not a clear rule for it, and the results can be pretty arbitrary. \n\nAnother problem arises: if the selection of the threshold is arbitrary, how do we *compare different models*? Here it is where the ROC curves come out!\n\nROC curves try to overcome this issue, taking into account all the possible scenarios given multiple thresholds. This allows us to estimate the performance of our model independently of the threshold you take.\n\n## How to create a ROC curve?\n\nTo create a ROC curve, the starting point is precisely the same information we used to display the density plot: a column with predicted probabilities and another with the real labels. Each row is an observation of the test set.\n\nOnce we have this information, we define as many thresholds [4] as observations found in the test set (plus Inf and -Inf). These values are defined by the probability of each observation.\n\nFurthermore, for each threshold value, all the probabilities above it will be identified as `abnormality = yes` and we count the number of True Positive (TP), True Negative (TN), but also, those observations predicted as abnormality = yes but actually are `no` (False Positive (FP)) and those predicted as `no` but actually are `yes` (False Negative (FN)). \n\nFinally, we need this information to calculate the values that will make up the ROC curve axis:\n\n- **Sensitivity** (also known as True positive rate). This metric reflects the number of positives in the test dataset that are correctly identified.\n- **Specificity** (also known as True negative rate). This metric measures the number of negatives in the test dataset that are correctly identified.\n\nIn both cases, a result of 1 is considered perfect.\n\nTo facilitate this, there are multiple packages in R to calculate the ROC curve. For this case, I am going to use the function `roc_curve` from the package `yardstick` which I recommend.\n\nCheck the output of the function `roc_curve`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we just need to specify the column with the labels (abnormality) and the predicted probabilities (pred_logistic_all)\nroc_logistic <- check_pred %>% roc_curve(abnormality, pred_logistic_all)\nroc_logistic %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf           0                1\n2     0.0179      0                1\n3     0.0292      0.0294           1\n4     0.0369      0.0588           1\n5     0.0372      0.0882           1\n6     0.0493      0.118            1\n```\n:::\n:::\n\n\nFinally, for the visualization, we only need to modify the specificity variable as `1 - specificity`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_logistic %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line() +\n geom_abline(linetype = 3) +\n theme_bw()\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Animated ROC curve\n\nTo build some intuition, we can see how to build the ROC curve while we define thresholds values in the density plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- check_pred %>% \n ggplot() +\n  geom_density(aes(x = pred_logistic_all, fill = abnormality), alpha = 0.5) +\n  geom_vline(data = roc_logistic %>% filter( .threshold != Inf) %>% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) +\n  transition_reveal(.threshold) +\n  theme_bw()\nb <- roc_logistic %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line() +\n  geom_point(colour = 'red', size = 3) +\n  transition_reveal(sensitivity) +\n geom_abline(linetype = 3) +\n theme_bw()\n\n# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition\na_gif <- animate(a, width = 440, height = 440)\nb_gif <- animate(b, width = 440, height = 440)\n\nnew_gif <- image_append(c(a_gif[1], b_gif[1]))\nfor(i in 2:100){\n combined <- image_append(c(a_gif[i], b_gif[i]))\n new_gif <- c(new_gif, combined)\n}\nnew_gif\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-8-1.gif)\n:::\n:::\n\n\n\n## Comparing six models\n\nAt the beginning of this post, we trained five models, each one with a different number of explanatory variables: one, two, three, four, five, and six.\n\nWe can easily display their probability distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison_six <- check_pred %>%\n pivot_longer(starts_with('pred'), names_to = 'model', values_to = 'prob') %>%\n mutate(model = fct_inorder(as.factor(model)))\ncomparison_six %>%\n ggplot(aes(prob)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  theme_bw() +\n  scale_fill_viridis_d() +\n facet_wrap(~ model)\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIn the example above, we observe that the overlapping between both distributions decreases as we increase the number of explanatory variables. In other words, since we increase the amount of useful information to discriminate between the two labels (*yes*, *no*), the predictive power of the model improves.\n\nBesides, we can plot their ROC curves:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison_six %>%\n group_by(model) %>%\n roc_curve(abnormality, prob) %>%\n  ggplot(aes(x = (1 - specificity), y = sensitivity)) +\n geom_line(aes(color = model)) +\n geom_abline(linetype = 3) +\n theme_bw()\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Animated comparison\n\nFinally, we can replicate the previous code and compare the six models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_comparison <- comparison_six %>%\n group_by(model) %>%\n roc_curve(abnormality, prob) %>% ungroup()\na <- comparison_six %>%\n ggplot(aes(prob)) +\n  geom_density(aes(fill = abnormality), alpha = 0.4) +\n  geom_vline(data = roc_comparison %>% filter(.threshold != Inf) %>% filter(.threshold != -Inf),\n        aes(xintercept = .threshold, group = .threshold)) +\n  theme_bw() +\n  scale_fill_viridis_d() +\n  transition_reveal(.threshold) +\n  facet_wrap(~ model)\nb <- roc_comparison %>%\n ggplot(aes(x = (1 - specificity), y = sensitivity, group = model)) +\n  geom_line(aes(color = model)) +\n  geom_point(colour = 'red', size = 3) +\n  transition_reveal(sensitivity) +\n  geom_abline(linetype = 3) +\n  theme_bw()\n# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition\na_gif <- animate(a, width = 440, height = 440)\nb_gif <- animate(b, width = 440, height = 440)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n```\n:::\n\n```{.r .cell-code}\nnew_gif <- image_append(c(a_gif[1], b_gif[1]))\nfor(i in 2:100){\n combined <- image_append(c(a_gif[i], b_gif[i]))\n new_gif <- c(new_gif, combined)\n}\nnew_gif\n```\n\n::: {.cell-output-display}\n![](roc-curves-animated-examples_files/figure-html/unnamed-chunk-11-1.gif)\n:::\n:::\n\n\n\n## Notes\n\n[1] The package `yardstick` as many other packages use the first level of the response variable factor as the \"event\". Therefore, the probability output determines how likely an observation belongs to the *first level of the factor* of the response variable. This behavior can be changed in the `yardstick` package global options.\n\n[2] The default output has a logit scale and needs to be transformed first to a probability value. This can be done automatically if we specify in the function `predict` the argument **type** as _prob_.\n\n[3] Apart from tree decisions, linear models can be also used in gradient boosting.\n\n[4] Because of this reason, ROC curves might not be appropriate to evaluate the performance of models on small test sets. \n\n",
    "supporting": [
      "roc-curves-animated-examples_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}